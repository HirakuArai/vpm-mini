# P4-B Evidence: Alerts & Reproducibility Pin (20250913_035306)

## Environment Configuration
- **APP_NS**: hyper-swarm
- **PROM_RELEASE**: vpm-mini-kube-prometheus-stack
- **KSVC_NAME**: hello-ai
- **Timestamp**: 2025-09-12 18:53:06 UTC

## Changes Applied

### 1. PrometheusRule Alerts
Created `monitoring/hello-ai-minimal` with the following alerts:
- **HelloAIServiceDown**: Critical alert when service is down for 2+ minutes
- **HelloAIHighLatency**: Warning when P95 latency > 500ms for 5+ minutes
- **HelloAIHighErrorRate**: Warning when 5xx error rate > 5% for 5+ minutes

### 2. Image Digest Pinning
| Component | Status | Details |
|-----------|--------|---------|
| Original Image | - | ghcr.io/hirakuarai/vpm-mini/hello-ai:latest |
| Digest Extraction | ❌ | No pod found |
| Pinned Image | ❌ | Not pinned |

## Verification Results
| Check | Status | Details |
|-------|--------|---------|
| Prometheus Targets | ⚠️ | hello-ai job in active targets |
| Alert Rules Loaded | ⚠️ | HelloAI* rules in /api/v1/rules |
| Active Alerts | ℹ️ | None firing (normal state) |
| Image Pinning | ❌ | Digest reference in ksvc spec |

## Alert Expressions

### HelloAIServiceDown
```promql
up{job=~".*hello-ai.*"} == 0
```
Fires when the metrics endpoint is unreachable for 2 minutes.

### HelloAIHighLatency
```promql
histogram_quantile(0.95, 
  sum by (le) (
    rate(http_request_duration_seconds_bucket{job=~".*hello-ai.*"}[5m])
  )
) > 0.5
```
Fires when P95 request latency exceeds 500ms for 5 minutes.

### HelloAIHighErrorRate
```promql
(
  sum(rate(http_requests_total{job=~".*hello-ai.*",status=~"5.."}[5m]))
  /
  sum(rate(http_requests_total{job=~".*hello-ai.*"}[5m]))
) * 100 > 5
```
Fires when 5xx error rate exceeds 5% for 5 minutes.

## Verification Commands
```bash
# Check PrometheusRule
kubectl -n monitoring get prometheusrule hello-ai-minimal -o yaml

# Verify rules loaded in Prometheus
kubectl -n monitoring port-forward svc/prometheus-operated 9090:9090
curl localhost:9090/api/v1/rules | jq '.data.groups[].rules[] | select(.name | test("HelloAI"))'

# Check alerts status
curl localhost:9090/api/v1/alerts | jq '.data.alerts[] | select(.labels.alertname | test("HelloAI"))'

# Verify image digest pinning
kubectl -n hyper-swarm get ksvc hello-ai -o jsonpath='{.spec.template.spec.containers[0].image}'
```

## Next Steps
1. **Tune Thresholds**: Adjust latency/error thresholds based on actual traffic patterns
2. **Alertmanager Integration**: Configure receivers after Secrets management (ESO/SOPS) is ready
3. **Dashboard Correlation**: Link alerts to Grafana panels for visual correlation
4. **Runbook Documentation**: Create runbooks for each alert with remediation steps

## Notes
- Alert evaluation interval: 30s for quick detection
- All alerts use job label pattern matching for flexibility
- Image digest pinning ensures reproducible deployments
- PrometheusRule uses release label for Prometheus Operator discovery

---
Generated by P4-B Alerts & Reproducibility Script
